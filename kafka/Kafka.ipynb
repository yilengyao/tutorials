{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "039bb9e1-cfbf-42ca-b139-bb6c028a728e",
   "metadata": {},
   "source": [
    "Kafka is a distrubuted streaming platform that was originally developed by LinkedIn and later open-sourced through the Apache Software Foundation. ka is designed to handle large volumees of data efficiently and provides funtionality that is fundatmentally centered around 3 main capabilities:\n",
    "1. `Publishing and Subscribing to Streams of Records`: Kafka allows you to publish (write) and subscribe to (read) streams of records, similar to message queue or enterprise messaging system. It is highly scalable and ensures that records are only consumed in the order they were published.\n",
    "2. `Storing Streams of Records`: Kafka stores a stream of records in categores called topics. Within a topic, records are stored in the order they are received. Kafka's storage system is essentially an append-only distributed log and is designed to be durable (records can be disk-based) and fast, primary because it benefits from sequential disk reads and writes.\n",
    "3. `Processing Streams of Records`: Kafka can be used for real-time data processing. This is facilitated by Kafka Streams, a client library for building applications and microservices where the input and output data are stored in Kafka clusters. Additionally, Kafka can connect to external systems (for data import/export( via Kafka Connect.\n",
    "\n",
    "Distributed streaming platform: is a system designed to handle the continuous inflow and processing of data across multiple machines in a network. It allows for the collection, storage, and real-time analysis of data streams from various sourcees. the platform ensures that data is reliably processed and is scalable, meaning it can handle increasing amounts of data by distributing the load across many servers. This makes it ideal for applications that require high throughput, low latency, and easy data integration, such as real-time analytics, monitoring and event-driven system.\n",
    "\n",
    "Append-only Distributed Log: A data storage system where records are continously added at the end of a log and never modified or deleted. This ensures that the data is immutable, providing a reliable and chronological record of events. Data is distributed across multiple servers to enhance fault tolerence and availability. This structure is fundamental for systems requiring a high-integrity, ordered record and transactions or events, like in financial services or real-time analytics. It forms the core of platforms like Apache Kafka, enabling efficient, scalable, and reliable data streaming and processing.\n",
    "\n",
    "### Core Concepts of Kafka\n",
    "- `Produce`: An entity (application, system) that sends records to a Kafka topic.\n",
    "- `Consumer`: An entity that reads records from a Kafka topic. Consumers subscribe to one or more Kafka topics and read the records in the order within each topic.\n",
    "- `Topic`: A category or feed name to which records are published. Topics in Kafka are always multi-subscriber; this is, a topic can have zero, one, or many consumers that subscribe to the data written to it.\n",
    "- `Broker`: A Kafka server that strores data and serves clients (producers and consumers). A Kafka cluster consists of one or more servers (Kafka brokers), which are responsible for receiving and storing records sent by producers and serving them to consumers.\n",
    "- `Partition`: Kafka topics are split into partitions, where each partition can be hosted on a different Kafka Broker, this allows the topic's load to be shared across multiple broker (and hence multiple servers). Each partition is an ordered, immutable sequence of records, and each record in a partition is assigned a unique offset.\n",
    "- `Offset`: A unique identifier of record within a partition. It denotes the position of a record within the partition.\n",
    "- `Consumer Groups`: Each consumer belongs to a consumer group. When multiple consumers are subscribed to a topic and belong to the same consumer group, each consumer in the group will typically read from a unique partition. This is how Kafka provides the scalability for consumers.\n",
    "\n",
    "The producers and consumes are completely decoupled, allowing each client to work independently.\n",
    "\n",
    "### Uses of Kafka\n",
    "Kafka is widely used for various data-intensive applications, including:\n",
    "- Real-Time Analytics: Kafka's real-time nature makes it suitable for operational monitoring, aggregating data from different sources.\n",
    "- Event Sourcing: Kafka can be used as a source of truth to record the state changes in a system in the order they occur.\n",
    "- Log Aggregation: Kafka provides a unified platform to collect logs from multiple servieces and make the available in a central place fro processing.\n",
    "- Stream Processing: With tools like Kafka Streams and KSQL, Kafka can be sued to process streams of data in real-time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19b2aba-b9bf-469f-93c3-70fcebddd8dc",
   "metadata": {},
   "source": [
    "## Kafka vs RabbitMQ vs SQS\n",
    "Kafka, RabbitMQ and SQS are all messaging systems, but they serve different purposes and have distinct architectures and feature sets. \n",
    "\n",
    "1. Apache Kafka\n",
    "- - Type: Distributed streaming platform\n",
    "  - Architecture: Kafka operates as a distributed commit log. It is designed to handle high output and scalable data streaming and allows storing and processing streams of records in real time.\n",
    "  - Use Cases: Kafka is ideal for large-scale message processing applications, real-time analytics, and data integration with high throughput and durability. It is used extensively for stream processing and event sourcing.\n",
    "  - Scaling: Kafka scales horizonatally with clusters, partitions, and topics, and can handle trillions of events a day.\n",
    "  - - Throughput: Kafka can sustain 100,000 to 2,000,000 messages per second.\n",
    "    - Message Size: Assuming message are small (around 1KB each), a single Kafka broker can handle thousands of messages per second. In a clustered setup with multiple brokers and well-partitioned topics, this number scan scale up significantly.\n",
    "    - Latency: Kafka has a typical latency of less than 10 milliseconds under high throughput scenarios.\n",
    "  - Reliability: Offers string durability and fault tolerance guarantees through replication.\n",
    "  - Order Guarantees: Guarantees order within a partition but not acrosss the system as a whole unless specifically configured.\n",
    "  - Consumer Management: Uses consumer groups to allow a group of processes to cooperatively handle partitions of a topic.\n",
    "\n",
    "2. RabbitMQ\n",
    "- - Type: Message broker.\n",
    "  - Architecture: traditional message-queuing system that supports several messaging protocols, primary AMQP. It operates with a broker architecture were messages are stored until consumers retrieves them.\n",
    "  - Use Cases: RabbitMQ is versatile and can be used for a variety of workloads ranging from simple queuing to complex routing and tracking of message delivery.\n",
    "  - Scaling: Can be clustered to achieve high availability but typically deals with lower throughput compared to Kafka.\n",
    "  - Reliability: Provides message durabilty, delivery acknoledgements, and persistent messaging to prevent message loss.\n",
    "  - - Throughput: RabbitMQ can handle about 20,000 to 50,000 messages pers second under optimal conditions with messages being small (around 1 KB).\n",
    "    - Message Size: As with Kafka, the size of messages can impact performance. RabbitMQ's performance is optimal with small messages.\n",
    "    - Latency: RabbitMQ generally offers low latency, but this can increase significantly under high load conditions or if disk persistence is required for message durabiltiy.\n",
    "  - Order Guarantees: Maintains order within a queue but does not have built-in partitioning as Kafka does.\n",
    "  - Consumer Management: Supports multiple consumers with manual message acknoledgment and delivery guarantees.\n",
    "\n",
    "3. Amazon SQS\n",
    "- - Type: Managed message queuing service.\n",
    "  - Architecture: Fully managed service by AWS that eliminates the need to maintain messaging software. It uses a simple web service interface to send messages between decoupled components of applications.\n",
    "  - Use cases: SQS is used for decoupling application components, ensuring that messages are delivered and processed asynchronously. It is suited for cloud-native applications that require integration with other AWS services.\n",
    "  - Scaling: Automatically scales with the workload and requires no user-managed infrastructures.\n",
    "  - - Throughput: nearly unlimited throughput.\n",
    "    - Message Size: 256 KB per message\n",
    "    - Low latency ranges to tens to hunderds of milisecond.\n",
    "  - Reliability: Provides high availability and redundancy with automatic scaling. Supports at-least-once message delivery and can be configured for FIFO (First-In-First-Out) to guarantee ordre.\n",
    "  - Ordering Guaranttes: Standard queues provide best-effor ordering, while FIFO queues guarantee ordering and exactly-once processing.\n",
    "  - Consumer Management: Does not offer consumer groups like Kafka. Each message can be processed by any consumer instance, with visibility timeouts to handle processsign acknowledgements.\n",
    "\n",
    "#### Summary\n",
    "- Kafka: excels in scenarios requiring high throughput, durable storage, and real-time processing.\n",
    "- RabbitMQ: is best for comples routing and traditional messaging and stroong support for various messaging protocols.\n",
    "- Amazon SQS: is ideal for cloud-native applications needing a simple, scalable queuign system without operational overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d709cd16-538a-491d-9867-5b85e7d93698",
   "metadata": {},
   "source": [
    "### Kafka's Message Delivery methods\n",
    "1. At-Least-Once Semantics\n",
    "- - Definition: A message is guaranteed to be delivered at least once to the consumer. There is a possibility that a message might be delivered more than once, i.e., duplicates can occur.\n",
    "  - Implementation: This is achieved by the consumer acknowledging receipt of a message back to the producer or message system. If the producer doesn't receive an acknowledgement (due to a timeout or network issue), it will resend the message.\n",
    "  - - Kafka Configuration: To achieve at-least-once semantics in Kafka, consumers must manually commit their offsets after processsing messages. If a consumer fails before committing the offset, the same message will be delivered gain.\n",
    "  - Use Case: Useful in scenarios where losing messages is unacceptable. However, the consumer must be able to handle or detect duplicate messages.\n",
    "\n",
    "2. At-Most-Once Semantics\n",
    "- - Definition: A message is delivered once or not at all, ensuring that it is never processed more than once. This does not guarantee that a message is delivered; it only guarantees that if it is delivered, it will not be delivered again.\n",
    "  - Implementation: The producer sends the message and then immediately considers it delivered, without waiting for acknowledgement. If a failure occurs in transmission, the message will not be resent.\n",
    "  - - Kafka Configuration: This can be configured by having the consumer automatically commit offsets before processing messagees. If the consumer crashes after commmitting but before processing, the message will not be reprocessed.\n",
    "  - Use Case: Suitable for situations where it's better not to process a message than to process it multiple times, such as in non-critical logging or telemetry data where missing some data is acceptable\n",
    "\n",
    "3. Exactly-Once Semantics (EOS)\n",
    "- - Definition: Ensures that each message is delivered and processed exactly once, eliminating the problems of lost or duplicate messages.\n",
    "  - Implementation: This is the most complex and resource-intensive to implement, typically involving transactions or additional coordination between the producer, message system, and consumer to track message delivery and processign state comprehensively.\n",
    "  - - Kafka Configuration: Kafka supports exactly-once semantics through its transactional API. By wrapping the production and consumption of messages in a transaction, Kafka ensurses that all messages are processed exactly once, even in the event of failures. this is achieved by coordinating the commit of consumer offsets with the production of messages, ensuring that either both actions are completed successfully or neither is.\n",
    "  - Use Case: Important in financial servies or billing systems wehre it is crucial that each operation is performed exactly once, such as transferrinh money between accounts or processign payment transactions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294b7e1b-fac4-4acb-ad23-dc440fd1ed9c",
   "metadata": {},
   "source": [
    "## Protocols\n",
    "Kafka primarily uses a custom TCP binary protocol rather than common messaging protocols like XMPP, JMS, or AMQP. This custom protocol is optimized for Kafka's architecture and helps in achieving high throughput and low latency that are characteristic of Kafka.\n",
    "\n",
    "## Message Storage\n",
    "Kafka's durability and reliability stem from how it stores messages.\n",
    "\n",
    "1. Disk-based Storage\n",
    "- - Kafka writes all messages on disk, which means they are stored on non-volitile storage. This approach ensures that even if a server or process crashes, the data is not lost and can be recovered once the system is back online.\n",
    "\n",
    "2. Log Structure\n",
    "- - Kafka orgaanizes data into topics and partitions. each partition is essentially an append-only log file. Messages are written sequentially to the end of a log, which is efficient because it avoids the overhead associated with random access storage patterns. This sequential disk access significantly enhanches both write and read performance.\n",
    " \n",
    "3. Replication\n",
    "- - To enhance the availability and fault tolerance, Kafka replicates each partition across multiple brokers (servers) in the cluster. This means that if one broker goes down, the data can still be served by another broker that has a copy of the same data.\n",
    " \n",
    "4. High Availability\n",
    "- - Kafka ensures high availability by allowing configurations where producers and consumers can automatically failover to other brokers if one fails. The commit log service ensures that any data written is not lost and can be fully recovered if a server associated with a broker fails.\n",
    " \n",
    "5. Batching\n",
    "- - Kafka increases throughput and efficiency by batchign multiple messages together. This reduces the number of I/O operations and network calls, speeding up the overall data processing. Batching is done both on the producer side when sending messages and on the broker side when writing messsages to disk.\n",
    " \n",
    "\n",
    "Append only log: Is a data structure where entries are sequentially added to the end of the log and are not modified or deleted after they have been written."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dd53e6-1f09-46ae-b28a-ef3a21893943",
   "metadata": {},
   "source": [
    "## Kafka Might be not a good fit for\n",
    "1. Infrequent Batch Processing: If you only need periodic summaries or batch processing of data, such as monthly or yearly aggregrates, maintaining a Kafka system running constantly might be overkill.\n",
    "2. Random Data Access: Kafka excels in sequential data read and write operations. But it is less efficient for random data access.\n",
    "3. Strict Message Order Across a Topic: While Kafka can guarantee order withing a partition, ensuring total order across a topic can be complex and restrictive, often requirigin a single partition and thus limiting scalability and parallel processing compabilities.\n",
    "4. Handling of Large Messages: Kafka's default message size limit is around 1 MB. Handling significantly larger messages can introduce memory pressure and affect performance. Other systems might handle large data blobs more efficiently if they are significant component of your workload.\n",
    "5. Specific Use Cases like IoT: Kafka is well-suited for managing high volumes of small messages, such as those generated by IoT devices. However, if IoT devices intermittenly transmit large batches of data due to connectivity availability, this might introduce bursts of data that could challenge Kafka's handling capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e097b0f9-dbf8-4bef-bd8f-49ce9865bfa7",
   "metadata": {},
   "source": [
    "## Messages\n",
    "A message (or record) is the basic unit of data and consists of a\n",
    "- key\n",
    "- value\n",
    "- timestamp\n",
    "\n",
    "A message can also include optional headers for additional metadata.\n",
    "\n",
    "This structure allows Kafka to efficiently manage and organize data flowing through the system. To send data, producer deliver messages to Kafka brokers, which manage the storage and distribution of these messages to consumers.\n",
    "\n",
    "## Brokers\n",
    "Brokers serve as the server-side components that handle the storage and transmission of messages. Brokes are essentially Kafka servers that manage the distribution and replication of message data across the cluster.\n",
    "\n",
    "To interact with Kafka, you can create topics using the `kafka-topics.sh` script. A topic is divided into partitions to allow data to be spread across multiple brokers for scalability and fault tolerance. The replication factor of a topic determines how many copies of each partition are make, enhancing data reliability.\n",
    "\n",
    "Partitions are component that allow the topics to be split across multiple brokers. When a topic is created, it can be configured with a specific number of partitions. Messages within a partition are ordered, and each message is assigned a sequential ID called an offset.\n",
    "\n",
    "A topic is a category or feed name wich messages are published. Topics are the way Kafka organizes and categorizes the data streams. Producers write their data into specific topics and consumers subscribe to topics to receive the messages they need. Topics are divided into partitions, which allows for the data within a topic to be split across multiple servers (brokers) for scalability and parallel processing.\n",
    "\n",
    "For example, creating a topic named \"kinaction_helloworld\" with 3 partitions and a replication factor of 3 means each partition's data is stored on 3 different brokers. This setup improves data availability and fault tolerance. The `--bootstrap-server` option is used to specify which Kafka broker command should communicate with to perform operations like creating of listing topics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc9a961-9ec7-4a8b-932d-0c9b817ac471",
   "metadata": {},
   "source": [
    "# Components of Kafka\n",
    "\n",
    "|Component|Role|\n",
    "|--|--|\n",
    "|Producer|Sends messages to Kafka|\n",
    "|Consumer|Retrieves messages from Kafka|\n",
    "|Topics|Logical name of where messages are stored in the broker|\n",
    "|ZooKeeper enemble|Helps maintain consensus in the cluster|\n",
    "|Broker|Handlese the commit log (how messages are stored on the disk)|\n",
    "\n",
    "\n",
    "## Sending messages to Kafka\n",
    "When you send data to Kafka cluster, you create a `ProducerRecord` object specifying the topic and the message (or data) you wish to sent. Using the `producer.send()` method, the message is sent asynchronously, allowing the handling of any potential errors via a callback mechanism.\n",
    "\n",
    "The `ProducerRecord` is a temporary object used by the producer client to encapsulate the details of the message (including the topic, value, and any optional headers)\n",
    "\n",
    "Workflow after you send a `ProducerRecord`\n",
    "1. Creation: The`ProducerRecord` is created in your client application. It holds the data you want to send to Kafka, such as the topic name, the message key, and the message value.\n",
    "2. Sending: when you call the `producer.send()` method, the `ProducerRecord` is passed to the Kafka producer client.\n",
    "3. Serialization: The producer client serialize the key and value of the `ProducerRecord` into bytes (according to the serializers defined in your producer configuration) to prepare them for transmission over the network.\n",
    "4. Partitioning: The producer client determines which partition of the specified topic should receive the record. This can be based on the key (if provided) using a consistent hashing mechanism or other partitioning logic.\n",
    "5. Batching: The producer may match multiple records together if the they destined for the same partition to improve efficiency and throughput.\n",
    "6. Transmission: The batched records are sent to the appropriate Kafka broker that manages the corresponding partitions.\n",
    "7. Storage: Once received by the Kafka broker, the data is written to the corresponding partition's log on the broker's file system. This is where the actual persistence of data occurs.\n",
    "\n",
    "The producer's role is primarily to prepare, batch, and send records to the Kafka brokers. Actual data storage is handled by the Kafka brokers, not by the producer. The producers are stateless in terms of data storage.\n",
    "\n",
    "## Consuming messages from Kafka\n",
    "In Kafka, consumers are tools used for retrieving messages from topics to which they subscribe. They continuously poll the topics for new data. The consumer setup involves subscribing to specific topics and then regularly polling these topics to fetch new messages. each fetched message can be processed, and its offset (a unique idetifier indicating the position of the record within the partition) can be manually committed, ensuring that each message is acknowledge and not re-read in case of consumer restart or failer.\n",
    "\n",
    "This process allows consuming application to continuously receive and process data from Kafaka integrating it into operational workflows. Kafka handles the data transmission efficiently but does not process or interpret data; this is the role of the consuming applications, which utilize the data to fulfill specifice business needs and objectives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3519b788-1a7f-4eb9-819b-f0e62c05a7ed",
   "metadata": {},
   "source": [
    "## Commit Log\n",
    "The Kafka commit log is an append-only log where messages are added sequentially. It is prominently used by consumers to track their position within the data stream offering offsets.\n",
    "\n",
    "Kafka's commit log is optimized for performance by utilizing the operating system's page cache rather than the JVM heap, which enables the processing of millions of messages rapidly and efficiently. The log's append-only nature not only ensures high data integrity but also simplifies the recovery process following broker failures. Messages are persistent in the log, allowing for robust data availability and supporting Kafka's high throughput capabilities.\n",
    "\n",
    "The data retention in the commit log is configurable, allowing organization to manage how long messages are kept based on size or time before potentially moving them to a permanent storage solution. This flexibility in data retention is crucial for managing disk space and processing needs effectively. Overall, the commit log's design and functionality are key to Kafka's performance, durability, and scalability.\n",
    "\n",
    "## Offsets\n",
    "Kafka offsets are critical identifiers used to track the position of a consumer within a partition of a topic. Understanding offsets is essential for effectively managing how messages are consumed and ensuring data is processed accurately.\n",
    "\n",
    "What are offsets?\n",
    "- Definition: An offset is a numeric value that uniquely identifies each message within a partition. It denotes the position of a record in a partitioned log.\n",
    "- Functionality: Offsets allow Kafka consumers to keep track of which messages had been consumed and which messages need to be read next. This tracking helps in maintaining the state of consumption across sessions.\n",
    "\n",
    "How Offsets Work\n",
    "- Sequential Nature: Messages in a Kafka partition have a specific order, and each message is assigned a unique, sequential offset starting from zero. As new messages are added to a partition, they receive incrementally higher offsets.\n",
    "- Consumer Tracking: Each consumer or consumer group tracks its offset independently. When a consumer reads a message, it advances its offset counter to the next value. This mechanism ensures that each consumer can resume reading from where it left off, even after failures or restarts.\n",
    "\n",
    "Management of Offsets\n",
    "- Storage: Offsets can be managed in 2 primary ways:\n",
    "- - Automatically by Kafka: By default, offsets are stored in a special Kafka topic named `__consumer_offsets`. This internal topic is replicated and highly available, ensuring that offset data is preserved even if some brokers fail.\n",
    "  - Externally by Consumers: Consumers can also manage their offsets externally, using a custom storage system like a database. This method gives applications more control over when and how offsets are committed but adds complexity to the consumer application.\n",
    "\n",
    "Committing Offsets\n",
    "- Automatic Commit: Kafka provides the option to automatically commit offsets at specified intervals. This is a straightforward approach but can lead to duplicate processing if a consumer crashes between a message consumption and the next automatic commit.\n",
    "- Manual Commit: Consumers can manually commit offsets either synchronously (waiting for the commit operation to complete before proceeding) or synchronously. Manual committing provides more precise control over when an offset is committed, which is useful in scenarios where exactly-once processing semantics are needed.\n",
    "\n",
    "Importance in Kafka Architecture\n",
    "- Fault Tolerance: By persisting offsets, Kafka ensures that message consumption can be resilient to consumer failures. Consumers can pick up reading from the last committed offset.\n",
    "- Scalability: Offsets facilitate independent and parallel processing of partitions by multiple consumers within the same consumer group, making Kafka highly scalable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb4758f-0f7d-4bc0-b1df-7b8b692586f7",
   "metadata": {},
   "source": [
    "## Write Ahead Log\n",
    "A Write-Ahead Log (WAL) is a fundamental concept used in database systems and certain types of file systems to ensure data integrity and durability. This mechanism involves writing changes to a log before they are applied to the database or file system. The primary purpose of a WAL is to provide a robust way to recover from crashes or system failures by replaying log entries to bring the system back to a consistent state.\n",
    "\n",
    "How Does a Write-Ahead Log Work?\n",
    "1. Pre-Write Logging:\n",
    "- - Before any changes are made to the actual data or database, the changes are first recorded in a log. This log entry includes all the information necessary to redo the operation.\n",
    "\n",
    "2. Commit:\n",
    "- - The transaction or change is considered committed once the log entry has been safely stored on durable storage (like a disk). Only after this step the actual data changes applied to the database.\n",
    " \n",
    "3. Recovery:\n",
    "- - If a system crash or failure occurs, the recovery process uses the WAL entries to determine which changes are successfully committed and need to be redone. Any transaction that was not logged as committed can be ignored, ensuring that the database remains consistent.\n",
    " \n",
    "Advantages of Write-Ahead Logging\n",
    "- Data Integrity: By logging changes before applying them, WAL ensures that even in the event of a crash, the database can be restored to a consistent state.\n",
    "- Durability: Committed changes are preserved in the log, which is stored on persistent storage, ensuring that data isn't lost.\n",
    "- Concurrency and Performance: WAL can help in managing concurrency as it allows the system to continue operations while the disk writes are batched and sequentially written, often leading to performance improvements over immediate writes to random places on disk.\n",
    "- Atomicity: Ensures that all parts of a transaction are completed sucessfully. If a crash happens during a transaction, the WAL contains enough information to either complete the transaction during the recoverty or roll it back.\n",
    "\n",
    "Use of WAL in Systems\n",
    "- Databases: Most relational database systems (like PostgreSQL, SQLite, and others) use WAL to handle transactions safely and efficiently. In databases, WAL is crucial for implementing tansactional integrity and recovery.\n",
    "- Distributed Systems: System like Apache Kafka and distributed databases use mechanisms similar to WAL to ensure data consistency across nodes and handle node failures gracefully.\n",
    "- File Systems: Some advanced file systems use write-ahead logging to manage metadata changes, ensuring the file system integrity afte crashes.\n",
    "- \n",
    "Write-ahead logging is a critical techniques in building reliable and robust data handling systems, ensuring that operations can survive failures without losing data or corrupting the database state. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
